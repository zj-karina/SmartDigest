"""Enhanced news digest pipeline with MongoDB integration and improved classification."""

import os
import time
import json
import numpy as np
from datetime import datetime
from typing import List, Dict, Any, Optional

from collectors.rss_collector import NewsCollector
from classifiers.enhanced_classifier import EnhancedNewsClassifier
from clustering.news_clusterer import NewsClusterer
from summarization.news_summarizer import NewsSummarizer
from database.db_manager import DatabaseManager

def convert_numpy_types(obj):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç numpy —Ç–∏–ø—ã –≤ –æ–±—ã—á–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif hasattr(obj, 'isoformat'):  # datetime objects
        return obj.isoformat()
    return obj

class EnhancedDigestPipeline:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π MongoDB –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π."""
    
    def __init__(self, 
                 mongodb_connection: Optional[str] = None,
                 openrouter_api_key: Optional[str] = None,
                 classification_method: str = 'bge_large',
                 clustering_eps: float = 0.35,
                 summarization_model: str = 'balanced',
                 relevance_threshold: float = 0.6,
                 use_database: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            mongodb_connection: –°—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MongoDB
            openrouter_api_key: API –∫–ª—é—á –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            classification_method: –ú–µ—Ç–æ–¥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            clustering_eps: –ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            summarization_model: –¢–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            relevance_threshold: –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            use_database: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        """
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Enhanced News Digest Pipeline")
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.collector = NewsCollector()
        self.classifier = EnhancedNewsClassifier(
            method=classification_method,
            relevance_threshold=relevance_threshold
        )
        self.clusterer = NewsClusterer()
        self.summarizer = NewsSummarizer(openrouter_api_key)
        
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        self.use_database = use_database
        self.db_manager = None
        if use_database:
            try:
                connection_string = (mongodb_connection or 
                                   os.getenv('MONGODB_CONNECTION_STRING'))
                self.db_manager = DatabaseManager(connection_string)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
                print("üìù –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
                self.use_database = False
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.classification_method = classification_method
        self.clustering_eps = clustering_eps
        self.summarization_model = summarization_model
        self.relevance_threshold = relevance_threshold
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.raw_news = []
        self.classified_news = []
        self.relevant_news = []
        self.irrelevant_news = []
        self.clusters = {}
        self.digest = {}
        
        print(f"‚úÖ Pipeline –≥–æ—Ç–æ–≤")
        print(f"   üìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {classification_method}")
        print(f"   üîç –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: eps={clustering_eps}")
        print(f"   üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {summarization_model}")
        print(f"   üíæ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {'‚úÖ' if self.use_database else '‚ùå'}")
        print(f"   üéØ –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {relevance_threshold}")
    
    def run_complete_pipeline(self, 
                             news_count: int = 50,
                             max_events: int = 7,
                             language: str = 'english',
                             save_results: bool = True,
                             save_to_db: bool = True) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            news_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–±–æ—Ä–∞
            max_events: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –≤ –¥–∞–π–¥–∂–µ—Å—Ç–µ
            language: –Ø–∑—ã–∫ –¥–∞–π–¥–∂–µ—Å—Ç–∞ ('english', 'russian')
            save_results: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã
            save_to_db: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞–π–¥–∂–µ—Å—Ç
        """
        pipeline_start = time.time()
        
        print("üöÄ –ó–∞–ø—É—Å–∫ Complete Pipeline")
        print("=" * 40)
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        print(f"   üì∞ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {news_count}")
        print(f"   üìä –ú–∞–∫—Å. —Å–æ–±—ã—Ç–∏–π: {max_events}")
        print(f"   üåç –Ø–∑—ã–∫: {language}")
        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î: {'‚úÖ' if save_to_db and self.use_database else '‚ùå'}")
        
        try:
            # –®–∞–≥ 1: –ß—Ç–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ MongoDB
            print(f"\nüì∞ –®–ê–ì 1: –ß—Ç–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ MongoDB")
            print("-" * 25)
            
            start_time = time.time()
            
            if self.use_database and self.db_manager:
                # –ß–∏—Ç–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ MongoDB
                self.raw_news = self.db_manager.get_articles(limit=news_count)
                print(f"üìä –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(self.raw_news)} —Å—Ç–∞—Ç–µ–π –∏–∑ MongoDB")
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç MongoDB –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                converted_news = []
                for article in self.raw_news:
                    converted_article = {
                        'title': article.get('title', ''),
                        'description': article.get('summary', ''),  # summary –≤–º–µ—Å—Ç–æ description
                        'url': article.get('url', ''),
                        'source': article.get('url', '').split('/')[2] if article.get('url') else 'unknown',
                        'published': article.get('publish_date'),
                        'language': 'english'  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
                    }
                    converted_news.append(converted_article)
                
                self.raw_news = converted_news
                collection_time = time.time() - start_time
                print(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(self.raw_news)} —Å—Ç–∞—Ç–µ–π –∑–∞ {collection_time:.1f}—Å")
            else:
                # Fallback –Ω–∞ RSS —Å–±–æ—Ä
                print("‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º RSS —Å–±–æ—Ä")
                self.raw_news = self.collector.collect_news(news_count)
                collection_time = time.time() - start_time
                print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(self.raw_news)} —Å—Ç–∞—Ç–µ–π –∑–∞ {collection_time:.1f}—Å")
            
            if not self.raw_news:
                print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
                return {}
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î (–æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ —Ç–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏—è)
            if self.use_database and save_to_db and False:  # –û—Ç–∫–ª—é—á–µ–Ω–æ
                print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –≤ –ë–î...")
                saved_ids = self.db_manager.save_articles(self.raw_news)
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(saved_ids)} —Å—Ç–∞—Ç–µ–π –≤ –ë–î")
            
            # –®–∞–≥ 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            print(f"\nüóÇÔ∏è –®–ê–ì 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
            print("-" * 25)
            
            start_time = time.time()
            self.classified_news = self.classifier.classify_batch(self.raw_news)
            classification_time = time.time() - start_time
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
            self.relevant_news = [news for news in self.classified_news if news.get('is_relevant', True)]
            self.irrelevant_news = [news for news in self.classified_news if not news.get('is_relevant', True)]
            
            print(f"‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {classification_time:.1f}—Å")
            print(f"üìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(self.relevant_news)}")
            print(f"üóëÔ∏è –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(self.irrelevant_news)}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            from collections import Counter
            category_stats = Counter([news.get('category', 'unknown') for news in self.relevant_news])
            print(f"üìà –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {dict(list(category_stats.most_common(5)))}")
            
            if not self.relevant_news:
                print("‚ùå –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
                return {}
            
            # –®–∞–≥ 3: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
            print(f"\nüîç –®–ê–ì 3: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
            print("-" * 25)
            
            start_time = time.time()
            self.clusters = self.clusterer.cluster_by_category(
                self.relevant_news, 
                eps=self.clustering_eps, 
                min_samples=2
            )
            clustering_time = time.time() - start_time
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            top_clusters = self.clusterer.get_top_clusters(
                min_size=2, 
                max_clusters=max_events
            )
            
            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {clustering_time:.1f}—Å")
            print(f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(top_clusters)} –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ–±—ã—Ç–∏–π")
            
            if not top_clusters:
                print("‚ùå –ó–Ω–∞—á–∏–º—ã–µ —Å–æ–±—ã—Ç–∏—è –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return {}
            
            # –®–∞–≥ 4: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            print(f"\nüìù –®–ê–ì 4: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
            print("-" * 25)
            
            if not self.summarizer.api_key:
                print("‚ö†Ô∏è –ù–µ—Ç API –∫–ª—é—á–∞ - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è fallback —Ä–µ–∑—é–º–µ")
            
            start_time = time.time()
            self.digest = self.summarizer.create_daily_digest(
                top_clusters,
                max_items=max_events,
                model_type=self.summarization_model,
                language=language
            )
            summarization_time = time.time() - start_time
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            self.digest.update({
                'articles_processed': len(self.classified_news),
                'relevant_articles': len(self.relevant_news),
                'irrelevant_articles': len(self.irrelevant_news),
                'processing_time': time.time() - pipeline_start,
                'pipeline_version': '2.0_enhanced'
            })
            
            total_time = time.time() - pipeline_start
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            print(f"\nüéâ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù")
            print("=" * 40)
            print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å")
            print(f"üì∞ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.classified_news)} —Å—Ç–∞—Ç–µ–π")
            print(f"‚úÖ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {len(self.relevant_news)} —Å—Ç–∞—Ç–µ–π")
            print(f"‚ùå –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {len(self.irrelevant_news)} —Å—Ç–∞—Ç–µ–π")
            print(f"üìä –°–æ–±—ã—Ç–∏–π –≤ –¥–∞–π–¥–∂–µ—Å—Ç–µ: {len(self.digest.get('items', []))}")
            print(f"üéØ –î–∞–π–¥–∂–µ—Å—Ç: {self.digest.get('title', '–ì–æ—Ç–æ–≤')}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if save_results:
                self.save_results()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î (–æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ —Ç–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏—è)
            if self.use_database and save_to_db and False:  # –û—Ç–∫–ª—é—á–µ–Ω–æ
                self.save_to_database()
            
            return self.digest
            
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def save_to_database(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
        if not self.use_database or not self.db_manager:
            return
        
        try:
            print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
            pipeline_config = {
                'classification_method': self.classification_method,
                'clustering_eps': self.clustering_eps,
                'summarization_model': self.summarization_model,
                'relevance_threshold': self.relevance_threshold,
                'version': '2.0_enhanced'
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞
            if self.digest:
                digest_id = self.db_manager.save_digest(self.digest, pipeline_config)
                print(f"‚úÖ –î–∞–π–¥–∂–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î —Å ID: {digest_id}")
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç–µ–π
            for news in self.classified_news:
                if hasattr(news, '_id'):  # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è —É–∂–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
                    self.db_manager.update_article_relevance(
                        news['_id'],
                        news.get('is_relevant', True),
                        news.get('relevance_confidence', 0.0)
                    )
            
            print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
    
    def save_results(self) -> Dict[str, str]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤ —Ñ–∞–π–ª—ã."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        try:
            os.makedirs('results', exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if self.classified_news:
                classification_file = f'results/classification_{timestamp}.json'
                classification_data = {
                    'metadata': {
                        'timestamp': timestamp,
                        'total_articles': len(self.classified_news),
                        'relevant_articles': len(self.relevant_news),
                        'irrelevant_articles': len(self.irrelevant_news),
                        'method': self.classification_method,
                        'relevance_threshold': self.relevance_threshold
                    },
                    'articles': convert_numpy_types(self.classified_news)
                }
                
                with open(classification_file, 'w', encoding='utf-8') as f:
                    json.dump(classification_data, f, indent=2, ensure_ascii=False)
                
                saved_files['classification'] = classification_file
                print(f"üìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {classification_file}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Ä–µ–∂–∏–º–∞ —Ç–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏—è
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            if self.clusters:
                clusters_file = f'results/clusters_{timestamp}.json'
                clusters_data = convert_numpy_types(self.clusters)
                
                with open(clusters_file, 'w', encoding='utf-8') as f:
                    json.dump(clusters_data, f, indent=2, ensure_ascii=False)
                
                saved_files['clusters'] = clusters_file
                print(f"üîç –ö–ª–∞—Å—Ç–µ—Ä—ã: {clusters_file}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞
            if self.digest:
                digest_file = f'results/digest_{timestamp}.json'
                digest_data = convert_numpy_types(self.digest)
                
                with open(digest_file, 'w', encoding='utf-8') as f:
                    json.dump(digest_data, f, indent=2, ensure_ascii=False)
                
                saved_files['digest'] = digest_file
                print(f"üìù –î–∞–π–¥–∂–µ—Å—Ç: {digest_file}")
                
                # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ Telegram
                telegram_file = f'results/telegram_{timestamp}.md'
                telegram_text = self.summarizer.format_for_telegram(self.digest)
                
                with open(telegram_file, 'w', encoding='utf-8') as f:
                    f.write(telegram_text)
                
                saved_files['telegram'] = telegram_file
                print(f"üì± Telegram: {telegram_file}")
            
            print(f"‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            return saved_files
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return {}
    
    def show_digest_preview(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–≤—å—é –¥–∞–π–¥–∂–µ—Å—Ç–∞."""
        if not self.digest:
            print("‚ùå –î–∞–π–¥–∂–µ—Å—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω")
            return
        
        print(f"\nüì∞ –ü–†–ï–í–¨–Æ –î–ê–ô–î–ñ–ï–°–¢–ê")
        print("=" * 50)
        print(f"üóìÔ∏è {self.digest.get('date', 'N/A')}")
        print(f"üì∞ {self.digest.get('title', '–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π')}")
        print("-" * 50)
        
        for i, item in enumerate(self.digest.get('items', []), 1):
            print(f"\n{i}. üìç {item.get('category', 'Unknown').upper()}")
            print(f"   {item.get('summary', '–ù–µ—Ç —Ä–µ–∑—é–º–µ')}")
            print(f"   üìä –°—Ç–∞—Ç–µ–π: {len(item.get('articles', []))}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞."""
        stats = {
            'pipeline': {
                'articles_collected': len(self.raw_news),
                'articles_classified': len(self.classified_news),
                'relevant_articles': len(self.relevant_news),
                'irrelevant_articles': len(self.irrelevant_news),
                'events_created': len(self.digest.get('items', [])),
                'classification_method': self.classification_method,
                'relevance_threshold': self.relevance_threshold
            }
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ –ë–î –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if self.use_database and self.db_manager:
            try:
                db_stats = self.db_manager.get_statistics()
                stats['database'] = db_stats
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î: {e}")
        
        return stats
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è."""
        if self.db_manager:
            self.db_manager.close() 
